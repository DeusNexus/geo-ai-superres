{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f356d4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# pip install matplotlib dotenv sentinelhub geopandas IProgress \n",
    "\n",
    "# %%\n",
    "import os\n",
    "import time\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from sentinelhub import SHConfig, SentinelHubCatalog, BBox, CRS, DataCollection\n",
    "from sentinelhub import MimeType, SentinelHubDownloadClient, SentinelHubRequest, bbox_to_dimensions, filter_times\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tifffile\n",
    "import json\n",
    "import uuid\n",
    "import glob\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configure Sentinel Hub\n",
    "config = SHConfig()\n",
    "config.sh_client_id = os.getenv(\"SENTINELHUB_CLIENT_ID2\")\n",
    "config.sh_client_secret = os.getenv(\"SENTINELHUB_CLIENT_SECRET2\")\n",
    "config.sh_base_url = 'https://sh.dataspace.copernicus.eu'\n",
    "config.sh_token_url = 'https://identity.dataspace.copernicus.eu/auth/realms/CDSE/protocol/openid-connect/token'\n",
    "\n",
    "# Initialize the catalog\n",
    "catalog = SentinelHubCatalog(config=config)\n",
    "\n",
    "# Helper function to normalize image based on Scene Classification Layer\n",
    "def normalize_by_scl(image, scl, valid_scl_values=[4, 5]):\n",
    "    \"\"\"\n",
    "    Normalize image using percentile scaling over valid SCL mask.\n",
    "    Returns normalized image, valid mask, list of p2, and p98 per band.\n",
    "    \"\"\"\n",
    "    valid_mask = np.isin(scl, valid_scl_values)\n",
    "    norm_image = np.zeros_like(image, dtype=np.float32)\n",
    "    norm_p2, norm_p98 = [], []\n",
    "\n",
    "    for band in range(image.shape[2]):\n",
    "        band_data = image[:, :, band]\n",
    "        band_valid = band_data[valid_mask]\n",
    "        if band_valid.size > 0:\n",
    "            p2, p98 = np.percentile(band_valid, (2, 98))\n",
    "        else:\n",
    "            p2, p98 = 0, 1\n",
    "        norm_p2.append(float(p2))\n",
    "        norm_p98.append(float(p98))\n",
    "        band_data = np.clip(band_data, p2, p98)\n",
    "        band_data = (band_data - p2) / (p98 - p2 + 1e-6)\n",
    "        norm_image[:, :, band] = band_data\n",
    "\n",
    "    return norm_image, valid_mask, norm_p2, norm_p98\n",
    "\n",
    "# Function to save a single image right after download\n",
    "def save_image(image, patch_id, timestamp, resolution, base_raw_dir, base_proc_dir, logfile_path, log_df, \n",
    "               band_list, original_band_res, patch_bbox, bands_units, bands_num, sampleType, cloud_cover_filter):\n",
    "    \"\"\"\n",
    "    Save a single downloaded Sentinel-2 image and its metadata.\n",
    "    \n",
    "    Args:\n",
    "        image: The raw image data\n",
    "        patch_id: ID of the patch\n",
    "        timestamp: Timestamp of the image\n",
    "        resolution: Resolution of the image\n",
    "        base_raw_dir: Directory to save raw images\n",
    "        base_proc_dir: Directory to save processed images\n",
    "        logfile_path: Path to the log file\n",
    "        log_df: DataFrame containing log information\n",
    "        band_list: List of bands in the image\n",
    "        original_band_res: Dictionary of original resolutions for each band\n",
    "        patch_bbox: Bounding box of the patch\n",
    "        bands_units: Units of the bands\n",
    "        bands_num: Number of bands\n",
    "        sampleType: Sample type of the image\n",
    "        cloud_cover_filter: Maximum cloud cover percentage\n",
    "        \n",
    "    Returns:\n",
    "        Updated log DataFrame\n",
    "    \"\"\"\n",
    "    if image is None:\n",
    "        print(f\"Skipping save for {patch_id}/{timestamp} - download failed.\")\n",
    "        return log_df\n",
    "        \n",
    "    # Generate unique filename\n",
    "    short_uuid = str(uuid.uuid4())[:8]\n",
    "    iso_time = timestamp.isoformat().replace(\":\", \"\").replace(\"-\", \"\")\n",
    "    base_filename = f\"{patch_id}_{iso_time}_res{resolution}\"\n",
    "\n",
    "    # File paths\n",
    "    raw_tiff_path = os.path.join(base_raw_dir, f\"{base_filename}.tiff\")\n",
    "    proc_tiff_path = os.path.join(base_proc_dir, f\"{base_filename}.tiff\")\n",
    "    json_path = os.path.join(base_proc_dir, f\"{base_filename}.json\")  # metadata with processed version\n",
    "\n",
    "    # Bounding box info for metadata\n",
    "    ul_lon = round(patch_bbox.lower_left[0], 4)\n",
    "    ul_lat = round(patch_bbox.upper_right[1], 4)\n",
    "    br_lon = round(patch_bbox.upper_right[0], 4)\n",
    "    br_lat = round(patch_bbox.lower_left[1], 4)\n",
    "\n",
    "    try:\n",
    "        # Convert to float32 for processing\n",
    "        image = image.astype(np.float32)\n",
    "\n",
    "        # Extract SCL and spectral bands\n",
    "        scl = image[:, :, -1]\n",
    "        spectral = image[:, :, :-1]\n",
    "\n",
    "        # Normalize spectral bands using valid SCL pixels\n",
    "        valid_scl_values = [4, 5]  # Vegetation and Bare Soils\n",
    "        norm_image, valid_mask, norm_p2, norm_p98 = normalize_by_scl(spectral, scl, valid_scl_values=valid_scl_values)\n",
    "\n",
    "        # Save raw TIFF (unmodified reflectance + SCL)\n",
    "        image_raw = np.transpose(image, (2, 0, 1))  # (bands, height, width)\n",
    "        tifffile.imwrite(raw_tiff_path, image_raw)\n",
    "        print(f\"Saved RAW: {raw_tiff_path}\")\n",
    "\n",
    "        # Save processed TIFF (normalized reflectance only, excluding SCL)\n",
    "        image_norm = np.transpose(norm_image, (2, 0, 1))  # (bands, height, width)\n",
    "        tifffile.imwrite(proc_tiff_path, image_norm)\n",
    "        print(f\"Saved PROCESSED: {proc_tiff_path}\")\n",
    "\n",
    "        # Save metadata\n",
    "        metadata = {\n",
    "            \"source\": \"SENTINEL2_L2A\",\n",
    "            \"patch_id\": patch_id,\n",
    "            \"uuid\": short_uuid,\n",
    "            \"timestamp\": timestamp.isoformat(),\n",
    "            \"resolution\": resolution,\n",
    "            \"sampleType\": sampleType,\n",
    "            \"bands_units\": bands_units,\n",
    "            \"bands_num\": bands_num,\n",
    "            \"bands_shape\": image.shape,\n",
    "            \"bands\": band_list,\n",
    "            \"bbox\": {\n",
    "                \"upper_left\": [ul_lat, ul_lon],\n",
    "                \"bottom_right\": [br_lat, br_lon]\n",
    "            },\n",
    "            \"original_band_resolutions\": original_band_res,\n",
    "            \"cloud_mask_applied\": True,\n",
    "            \"max_search_cloud_cover\": cloud_cover_filter,\n",
    "            \"normalization\": {\n",
    "                \"percentiles\": [2, 98],\n",
    "                \"p2\": norm_p2,\n",
    "                \"p98\": norm_p98\n",
    "            },\n",
    "            \"file_raw\": os.path.basename(raw_tiff_path),\n",
    "            \"file_processed\": os.path.basename(proc_tiff_path),\n",
    "        }\n",
    "\n",
    "        with open(json_path, \"w\") as f:\n",
    "            json.dump(metadata, f, indent=4)\n",
    "            print(f\"Saved metadata: {json_path}\")\n",
    "\n",
    "        # Append log entry for this image\n",
    "        log_df = pd.concat([log_df, pd.DataFrame([{\n",
    "            \"patch_id\": patch_id,\n",
    "            \"uuid\": short_uuid,\n",
    "            \"timestamp\": timestamp.isoformat(),\n",
    "            \"file_processed\": os.path.basename(proc_tiff_path),\n",
    "            \"status\": \"success\"\n",
    "        }])], ignore_index=True)\n",
    "\n",
    "        # Save log immediately\n",
    "        log_df.to_csv(logfile_path, index=False)\n",
    "        print(f\"Logged patch {patch_id} to {logfile_path}\")\n",
    "\n",
    "        return log_df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving image: {e}\")\n",
    "        # Log the failure\n",
    "        log_df = pd.concat([log_df, pd.DataFrame([{\n",
    "            \"patch_id\": patch_id,\n",
    "            \"uuid\": short_uuid,\n",
    "            \"timestamp\": timestamp.isoformat(),\n",
    "            \"file_processed\": os.path.basename(proc_tiff_path) if os.path.exists(proc_tiff_path) else None,\n",
    "            \"status\": f\"error: {str(e)[:100]}\"\n",
    "        }])], ignore_index=True)\n",
    "        log_df.to_csv(logfile_path, index=False)\n",
    "        return log_df\n",
    "\n",
    "#################################\n",
    "######### PATCHES LOGIC #########\n",
    "#################################\n",
    "# Load patches\n",
    "patches_gdf = gpd.read_file(\"../data/patches/highlighted_patches.geojson\")\n",
    "print(f\"Loaded {len(patches_gdf)} patches\")\n",
    "\n",
    "# Ensure correct CRS\n",
    "assert patches_gdf.crs.to_epsg() == 3857, \"Expected EPSG:3857 in patches.geojson\"\n",
    "\n",
    "# Extract numeric index from patch_id for sorting\n",
    "patches_gdf[\"patch_index\"] = patches_gdf[\"patch_id\"].str.extract(r'patch_(\\d{5})')[0].astype(int)\n",
    "patches_gdf = patches_gdf.sort_values(\"patch_index\").reset_index(drop=True)\n",
    "\n",
    "# Resume from specific patch\n",
    "start_from_patch = \"patch_02149_fa728d65\"  # ← Change this to resume from another\n",
    "if start_from_patch in patches_gdf[\"patch_id\"].values:\n",
    "    start_index = patches_gdf[patches_gdf[\"patch_id\"] == start_from_patch].index[0]\n",
    "    patches_gdf = patches_gdf.iloc[start_index:].reset_index(drop=True)\n",
    "    print(f\"Resuming from patch: {start_from_patch}\")\n",
    "else:\n",
    "    print(f\"Start patch '{start_from_patch}' not found. Starting from the beginning.\")\n",
    "\n",
    "# Limit number of patches processed\n",
    "patch_limit = None # Set to None to process all patches, or a specific number\n",
    "patches_gdf = patches_gdf.head(patch_limit)\n",
    "print(f\"Processing {len(patches_gdf)} patches...\")\n",
    "\n",
    "\n",
    "######################## SPECIFY PARAMETERS #############################\n",
    "#########################################################################\n",
    "##########################################################################\n",
    "# Define the time range for the search\n",
    "time_interval = (\"2010-01-01\", \"2030-12-31\")\n",
    "cloud_cover_filter = 20  # max % cloud cover allowed\n",
    "\n",
    "# Specify the bands to download\n",
    "bands_num = 13\n",
    "bands_units = \"DN\"\n",
    "sampleType = \"uint16\"\n",
    "resolution = 1024 #10 # Band Resolution\n",
    "\n",
    "# Define band list and original resolutions\n",
    "band_list = [\"B01\", \"B02\", \"B03\", \"B04\", \"B05\", \"B06\", \"B07\", \"B08\", \"B8A\", \"B09\", \"B11\", \"B12\", \"SCL\"]\n",
    "original_band_res = {\n",
    "    \"B01\": 60, \"B02\": 10, \"B03\": 10, \"B04\": 10,\n",
    "    \"B05\": 20, \"B06\": 20, \"B07\": 20,\n",
    "    \"B08\": 10, \"B8A\": 20, \"B09\": 60,\n",
    "    \"B11\": 20, \"B12\": 20, \"SCL\": 20\n",
    "}\n",
    "\n",
    "# Set paths\n",
    "processed_dir = \"../data/processed/sentinel2\"\n",
    "logfile_path = \"../data/logs/patch_download_log.csv\"\n",
    "os.makedirs(os.path.dirname(logfile_path), exist_ok=True)\n",
    "\n",
    "# Define the directory to save the images\n",
    "save_dir = \"../data/raw/sentinel2\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Configuration\n",
    "pixel_scaling = 1\n",
    "base_raw_dir = \"../data/raw/sentinel2\"\n",
    "base_proc_dir = \"../data/processed/sentinel2\"\n",
    "os.makedirs(base_raw_dir, exist_ok=True)\n",
    "os.makedirs(base_proc_dir, exist_ok=True)\n",
    "##########################################################################\n",
    "##########################################################################\n",
    "##########################################################################\n",
    "\n",
    "# Function to check if an image exists for a specific patch and timestamp\n",
    "def check_image_exists(patch_id, timestamp, resolution, raw_dir, proc_dir):\n",
    "    \"\"\"Check if an image already exists in either raw or processed directories.\"\"\"\n",
    "    iso_time = timestamp.isoformat().replace(\":\", \"\").replace(\"-\", \"\")\n",
    "    base_filename = f\"{patch_id}_{iso_time}_res{resolution}\"\n",
    "    \n",
    "    raw_path = os.path.join(raw_dir, f\"{base_filename}.tiff\")\n",
    "    proc_path = os.path.join(proc_dir, f\"{base_filename}.tiff\")\n",
    "    json_path = os.path.join(proc_dir, f\"{base_filename}.json\")\n",
    "    \n",
    "    # Check if both raw and processed files exist\n",
    "    return os.path.exists(raw_path) and os.path.exists(proc_path) and os.path.exists(json_path)\n",
    "\n",
    "# List already processed patch_ids from saved files\n",
    "existing_files = glob.glob(os.path.join(processed_dir, \"patch_*.json\"))\n",
    "processed_patch_ids = set()\n",
    "\n",
    "for path in existing_files:\n",
    "    fname = os.path.basename(path)\n",
    "    if fname.startswith(\"patch_\") and fname.endswith(\".json\"):\n",
    "        patch_id = fname.split(\"_\")[1]  # e.g., patch_02149_xxx → 02149\n",
    "        full_id = \"_\".join(fname.split(\"_\")[1:3])  # e.g., 02149_xxx\n",
    "        processed_patch_ids.add(f\"patch_{full_id}\")\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(os.path.dirname(logfile_path), exist_ok=True)\n",
    "\n",
    "# Load or initialize patch log\n",
    "if os.path.exists(logfile_path):\n",
    "    log_df = pd.read_csv(logfile_path)\n",
    "    logged_patch_ids = set(log_df[\"patch_id\"])\n",
    "    print(f\"Loaded log with {len(log_df)} entries\")\n",
    "else:\n",
    "    log_df = pd.DataFrame(columns=[\"patch_id\", \"uuid\", \"timestamp\", \"file_processed\", \"status\"])\n",
    "    logged_patch_ids = set()\n",
    "    print(\"Initialized new patch log\")\n",
    "\n",
    "# Define the evalscript for downloading Sentinel-2 images\n",
    "all_bands_evalscript = f\"\"\"\n",
    "//VERSION=3\n",
    "\n",
    "function setup() {{\n",
    "    return {{\n",
    "        input: [{{\n",
    "            bands: [\"B01\", \"B02\", \"B03\", \"B04\", \"B05\", \"B06\", \"B07\", \"B08\", \"B8A\", \"B09\", \"B11\", \"B12\", \"SCL\"],\n",
    "            units: \"{bands_units}\"\n",
    "        }}],\n",
    "        output: {{\n",
    "            bands: {bands_num},\n",
    "            sampleType: \"{sampleType}\"\n",
    "        }}\n",
    "    }};\n",
    "}}\n",
    "\n",
    "function evaluatePixel(sample) {{\n",
    "    return [\n",
    "        sample.B01, sample.B02, sample.B03, sample.B04, sample.B05, sample.B06,\n",
    "        sample.B07, sample.B08, sample.B8A, sample.B09, sample.B11, sample.B12,\n",
    "        sample.SCL\n",
    "    ];\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "##########################\n",
    "####### MAIN LOOP ########\n",
    "##########################\n",
    "\n",
    "# Loop over patches\n",
    "for idx, row in patches_gdf.iterrows():\n",
    "    patch_id = row[\"patch_id\"]\n",
    "    geom = row[\"geometry\"]\n",
    "\n",
    "    if patch_id in processed_patch_ids:\n",
    "        print(f\"Patch {patch_id} already processed. Skipping.\")\n",
    "        log_df = pd.concat([log_df, pd.DataFrame([{\n",
    "            \"patch_id\": patch_id,\n",
    "            \"uuid\": None,\n",
    "            \"timestamp\": None,\n",
    "            \"file_processed\": None,\n",
    "            \"status\": \"skipped\"\n",
    "        }])], ignore_index=True)\n",
    "        log_df.to_csv(logfile_path, index=False)\n",
    "        continue\n",
    "\n",
    "    if geom.geom_type != \"Polygon\":\n",
    "        print(f\"Skipping non-polygon geometry in patch {patch_id}\")\n",
    "        log_df = pd.concat([log_df, pd.DataFrame([{\n",
    "            \"patch_id\": patch_id,\n",
    "            \"uuid\": None,\n",
    "            \"timestamp\": None,\n",
    "            \"file_processed\": None,\n",
    "            \"status\": \"non-polygon\",\n",
    "        }])], ignore_index=True)\n",
    "        log_df.to_csv(logfile_path, index=False)\n",
    "        continue\n",
    "\n",
    "    # Convert geometry bounds to a BBox in Web Mercator\n",
    "    patch_bbox = BBox(bbox=geom.bounds, crs=CRS.POP_WEB)  # EPSG:3857 == POP_WEB\n",
    "\n",
    "    print(f\"\\nProcessing patch {patch_id} with patch_bbox: {patch_bbox}\")\n",
    "\n",
    "    # Perform a search within the bounding box and time range\n",
    "    search_iterator = catalog.search(\n",
    "        DataCollection.SENTINEL2_L2A,\n",
    "        bbox=patch_bbox,\n",
    "        time=time_interval,\n",
    "        filter= f\"eo:cloud_cover < {cloud_cover_filter}\",  # Optional filter for cloud cover\n",
    "        fields={\"include\": [\"id\", \"properties.datetime\", \"properties.eo:cloud_cover\", \"geometry\"], \"exclude\": []},\n",
    "    )\n",
    "\n",
    "    # Convert the results to a list of features\n",
    "    features = list(search_iterator)\n",
    "\n",
    "    # Create a GeoDataFrame from the features\n",
    "    results_gdf = gpd.GeoDataFrame.from_features(features)\n",
    "\n",
    "    # Display the results\n",
    "    print(\"Total number of results:\", len(features))\n",
    "    if len(features) > 0:\n",
    "        print(results_gdf)\n",
    "\n",
    "    # Find unique acquisitions\n",
    "    time_difference = dt.timedelta(hours=1)\n",
    "    all_timestamps = search_iterator.get_timestamps()\n",
    "    unique_acquisitions = filter_times(all_timestamps, time_difference)\n",
    "\n",
    "    if not unique_acquisitions:\n",
    "        print(f\"No acquisitions found for patch {patch_id}. Skipping.\")\n",
    "        log_df = pd.concat([log_df, pd.DataFrame([{\n",
    "            \"patch_id\": patch_id,\n",
    "            \"uuid\": None,\n",
    "            \"timestamp\": None,\n",
    "            \"file_processed\": None,\n",
    "            \"status\": \"no_acquisitions\"\n",
    "        }])], ignore_index=True)\n",
    "        log_df.to_csv(logfile_path, index=False)\n",
    "        continue\n",
    "\n",
    "    # print('unique_acquisitions: ', unique_acquisitions)\n",
    "\n",
    "    # Initialize Sentinel Hub client\n",
    "    client = SentinelHubDownloadClient(config=config)\n",
    "    \n",
    "    # Process each unique acquisition\n",
    "    for timestamp in unique_acquisitions:\n",
    "        # Check if this image already exists in both raw and processed directories\n",
    "        if check_image_exists(patch_id, timestamp, resolution, base_raw_dir, base_proc_dir):\n",
    "            print(f\"Image for {patch_id} at {timestamp} already exists. Skipping.\")\n",
    "            continue\n",
    "            \n",
    "        # Create request for this timestamp\n",
    "        request = SentinelHubRequest(\n",
    "            evalscript=all_bands_evalscript,\n",
    "            input_data=[\n",
    "                SentinelHubRequest.input_data(\n",
    "                    data_collection=DataCollection.SENTINEL2_L2A.define_from(\"s2l2a\", service_url=config.sh_base_url),\n",
    "                    time_interval=(timestamp - time_difference, timestamp + time_difference),\n",
    "                )\n",
    "            ],\n",
    "            responses=[SentinelHubRequest.output_response(\"default\", MimeType.TIFF)],\n",
    "            bbox=patch_bbox,\n",
    "            size=(resolution, resolution),  # Force fixed pixel dimensions\n",
    "            config=config,\n",
    "        )\n",
    "        \n",
    "        # Download with retry logic\n",
    "        max_retries = 5\n",
    "        image_data = None\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                print(f\"Downloading image for {patch_id} at {timestamp.isoformat()} (attempt {attempt+1}/{max_retries})...\")\n",
    "                download_request = request.download_list[0]\n",
    "                image_data = client.download(download_request)\n",
    "                break\n",
    "            except Exception as e:\n",
    "                is_throttle = any(code in str(e) for code in [\"429\", \"503\"])\n",
    "                wait = 2 ** attempt\n",
    "                print(f\"⚠️ Attempt {attempt+1} failed: {e}\")\n",
    "                if is_throttle:\n",
    "                    print(f\"⏳ Throttled! Waiting {wait}s and retrying...\")\n",
    "                    time.sleep(wait)\n",
    "                elif attempt < max_retries - 1:\n",
    "                    print(f\"Waiting {wait}s and retrying...\")\n",
    "                    time.sleep(wait)\n",
    "                else:\n",
    "                    print(f\"⛔️ All attempts failed for {timestamp}. Skipping.\")\n",
    "                    # Log failure\n",
    "                    log_df = pd.concat([log_df, pd.DataFrame([{\n",
    "                        \"patch_id\": patch_id,\n",
    "                        \"uuid\": str(uuid.uuid4())[:8],\n",
    "                        \"timestamp\": timestamp.isoformat(),\n",
    "                        \"file_processed\": None,\n",
    "                        \"status\": f\"download_failed: {str(e)[:100]}\"\n",
    "                    }])], ignore_index=True)\n",
    "                    log_df.to_csv(logfile_path, index=False)\n",
    "        \n",
    "        # Save the image if download was successful\n",
    "        if image_data is not None:\n",
    "            print(f\"Successfully downloaded image for {patch_id} at {timestamp.isoformat()}\")\n",
    "            if len(image_data.shape) == 3:  # Ensure we have a valid 3D array\n",
    "                log_df = save_image(\n",
    "                    image_data, patch_id, timestamp, resolution, \n",
    "                    base_raw_dir, base_proc_dir, logfile_path, log_df,\n",
    "                    band_list, original_band_res, patch_bbox, bands_units, \n",
    "                    bands_num, sampleType, cloud_cover_filter\n",
    "                )\n",
    "            else:\n",
    "                print(f\"⚠️ Invalid image data shape: {image_data.shape}\")\n",
    "                \n",
    "        # Throttle between requests to avoid hitting rate limits\n",
    "        time.sleep(1.0)\n",
    "\n",
    "    print(f\"Finished processing patch {patch_id}\")\n",
    "\n",
    "print(\"Processing complete!\")\n",
    "# %%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
