{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5eecfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install matplotlib dotenv sentinelhub geopandas IProgress "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f356d4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from sentinelhub import SHConfig, SentinelHubCatalog, BBox, CRS, DataCollection\n",
    "from sentinelhub import MimeType, SentinelHubDownloadClient, SentinelHubRequest, bbox_to_dimensions, filter_times\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tifffile\n",
    "import json\n",
    "import uuid\n",
    "import glob\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configure Sentinel Hub\n",
    "config = SHConfig()\n",
    "config.sh_client_id = os.getenv(\"SENTINELHUB_CLIENT_ID\")\n",
    "config.sh_client_secret = os.getenv(\"SENTINELHUB_CLIENT_SECRET\")\n",
    "config.sh_base_url = 'https://sh.dataspace.copernicus.eu'\n",
    "config.sh_token_url = 'https://identity.dataspace.copernicus.eu/auth/realms/CDSE/protocol/openid-connect/token'\n",
    "\n",
    "# Initialize the catalog\n",
    "catalog = SentinelHubCatalog(config=config)\n",
    "\n",
    "#################################\n",
    "######### PATCHES LOGIC #########\n",
    "#################################\n",
    "# Load patches\n",
    "patches_gdf = gpd.read_file(\"../data/patches/highlighted_patches.geojson\")\n",
    "print(f\"Loaded {len(patches_gdf)} patches\")\n",
    "\n",
    "# Ensure correct CRS\n",
    "assert patches_gdf.crs.to_epsg() == 3857, \"Expected EPSG:3857 in patches.geojson\"\n",
    "\n",
    "# Extract numeric index from patch_id for sorting\n",
    "patches_gdf[\"patch_index\"] = patches_gdf[\"patch_id\"].str.extract(r'patch_(\\d{5})')[0].astype(int)\n",
    "patches_gdf = patches_gdf.sort_values(\"patch_index\").reset_index(drop=True)\n",
    "\n",
    "# Resume from specific patch\n",
    "start_from_patch = \"patch_02149_fa728d65\"  # ← Change this to resume from another\n",
    "if start_from_patch in patches_gdf[\"patch_id\"].values:\n",
    "    start_index = patches_gdf[patches_gdf[\"patch_id\"] == start_from_patch].index[0]\n",
    "    patches_gdf = patches_gdf.iloc[start_index:].reset_index(drop=True)\n",
    "    print(f\"Resuming from patch: {start_from_patch}\")\n",
    "else:\n",
    "    print(f\"Start patch '{start_from_patch}' not found. Starting from the beginning.\")\n",
    "\n",
    "# Limit number of patches processed\n",
    "patch_limit = 2\n",
    "patches_gdf = patches_gdf.head(patch_limit)\n",
    "print(f\"Processing {len(patches_gdf)} patches...\")\n",
    "\n",
    "# Set paths\n",
    "processed_dir = \"../data/processed/sentinel2\"\n",
    "logfile_path = \"../data/processed/patch_download_log.csv\"\n",
    "\n",
    "# List already processed patch_ids from saved files\n",
    "existing_files = glob.glob(os.path.join(processed_dir, \"patch_*.json\"))\n",
    "processed_patch_ids = set()\n",
    "\n",
    "for path in existing_files:\n",
    "    fname = os.path.basename(path)\n",
    "    if fname.startswith(\"patch_\") and fname.endswith(\".json\"):\n",
    "        patch_id = fname.split(\"_\")[1]  # e.g., patch_02149_xxx → 02149\n",
    "        full_id = \"_\".join(fname.split(\"_\")[1:3])  # e.g., 02149_xxx\n",
    "        processed_patch_ids.add(f\"patch_{full_id}\")\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(os.path.dirname(logfile_path), exist_ok=True)\n",
    "\n",
    "# Load or initialize patch log\n",
    "if os.path.exists(logfile_path):\n",
    "    log_df = pd.read_csv(logfile_path)\n",
    "    logged_patch_ids = set(log_df[\"patch_id\"])\n",
    "    print(f\"Loaded log with {len(log_df)} entries\")\n",
    "else:\n",
    "    log_df = pd.DataFrame(columns=[\"patch_id\", \"uuid\", \"timestamp\", \"file_processed\", \"status\"])\n",
    "    logged_patch_ids = set()\n",
    "    print(\"Initialized new patch log\")\n",
    "\n",
    "\n",
    "# #### REPLACED BY PATCHES ####\n",
    "# # Define the bounding box\n",
    "# # bbox = BBox(bbox=[115.24026, -8.52927, 115.28474, -8.48453], crs=CRS.WGS84)\n",
    "# bbox = BBox(bbox=[115.1716, -8.5968, 115.3534, -8.4170], crs=CRS.WGS84)\n",
    "\n",
    "##########################\n",
    "####### LOOOP ############\n",
    "##########################\n",
    "\n",
    "# Loop over patches\n",
    "for idx, row in patches_gdf.iterrows():\n",
    "    patch_id = row[\"patch_id\"]\n",
    "    geom = row[\"geometry\"]\n",
    "\n",
    "    if patch_id in processed_patch_ids:\n",
    "        print(f\"Patch {patch_id} already processed. Skipping.\")\n",
    "        log_df = pd.concat([log_df, pd.DataFrame([{\n",
    "            \"patch_id\": patch_id,\n",
    "            \"uuid\": None,\n",
    "            \"timestamp\": None,\n",
    "            \"file_processed\": None,\n",
    "            \"status\": \"skipped\"\n",
    "        }])], ignore_index=True)\n",
    "        log_df.to_csv(logfile_path, index=False)\n",
    "        continue\n",
    "\n",
    "    if geom.geom_type != \"Polygon\":\n",
    "        print(f\"Skipping non-polygon geometry in patch {patch_id}\")\n",
    "        log_df = pd.concat([log_df, pd.DataFrame([{\n",
    "            \"patch_id\": patch_id,\n",
    "            \"uuid\": None,\n",
    "            \"timestamp\": None,\n",
    "            \"file_processed\": None,\n",
    "            \"status\": \"non-polygon\",\n",
    "        }])], ignore_index=True)\n",
    "        log_df.to_csv(logfile_path, index=False)\n",
    "        continue\n",
    "\n",
    "    # Convert geometry bounds to a BBox in Web Mercator\n",
    "    patch_bbox = BBox(bbox=geom.bounds, crs=CRS.POP_WEB)  # EPSG:3857 == POP_WEB\n",
    "\n",
    "    print(f\"\\nProcessing patch {patch_id} with patch_bbox: {patch_bbox}\")\n",
    "\n",
    "    # Run existing code logic here, replacing the old static `bbox` with `patch_bbox`\n",
    "    # E.g., search, get timestamps, create requests, download, save, etc.\n",
    "\n",
    "    # Define the time range for the search\n",
    "    time_interval = (\"2010-01-01\", \"2026-12-31\")\n",
    "    cloud_cover_filter = 2  # max % cloud cover allowed\n",
    "\n",
    "    # Perform a search within the bounding box and time range\n",
    "    search_iterator = catalog.search(\n",
    "        DataCollection.SENTINEL2_L2A,\n",
    "        bbox=patch_bbox,\n",
    "        time=time_interval,\n",
    "        filter= f\"eo:cloud_cover < {cloud_cover_filter}\",  # Optional filter for cloud cover\n",
    "        fields={\"include\": [\"id\", \"properties.datetime\", \"properties.eo:cloud_cover\", \"geometry\"], \"exclude\": []},\n",
    "    )\n",
    "\n",
    "    # Convert the results to a list of features\n",
    "    features = list(search_iterator)\n",
    "\n",
    "    # Create a GeoDataFrame from the features\n",
    "    results_gdf = gpd.GeoDataFrame.from_features(features)\n",
    "\n",
    "    # Display the results\n",
    "    print(\"Total number of results:\", len(features))\n",
    "    print(results_gdf)\n",
    "\n",
    "    # Find unique acquisitions\n",
    "    time_difference = dt.timedelta(hours=1)\n",
    "\n",
    "    all_timestamps = search_iterator.get_timestamps()\n",
    "    unique_acquisitions = filter_times(all_timestamps, time_difference)\n",
    "\n",
    "    if not unique_acquisitions:\n",
    "        print(f\"No acquisitions found for patch {patch_id}. Skipping.\")\n",
    "        # ✅ ADD THIS:\n",
    "        log_df = pd.concat([log_df, pd.DataFrame([{\n",
    "            \"patch_id\": patch_id,\n",
    "            \"uuid\": None,\n",
    "            \"timestamp\": None,\n",
    "            \"file_processed\": None,\n",
    "            \"status\": \"no_acquisitions\"\n",
    "        }])], ignore_index=True)\n",
    "        log_df.to_csv(logfile_path, index=False)\n",
    "        continue\n",
    "\n",
    "    print('unique_acquisitions: ',unique_acquisitions)\n",
    "\n",
    "    # Define the directory to save the images\n",
    "    save_dir = \"../data/raw/sentinel2\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Specify the bands to download\n",
    "    bands_num = 13\n",
    "    bands_units = \"DN\"\n",
    "    sampleType = \"uint16\"\n",
    "\n",
    "    # Define the resolution (max resolution is 10m for Sentinel-2)\n",
    "    # Note: Sentinel-2 has different resolutions for different bands (10m, 20m, 60m)\n",
    "    # Here we set the resolution to 100m for all bands, but you can adjust this as needed\n",
    "    # For example, if you want to download all bands at 20m resolution, set resolution = 20\n",
    "    # Supported resolution for B01, B02, B03, B04, B05, B06, B07, B08, B8A is 10m\n",
    "    # Supported resolution for B11, B12 is 20m\n",
    "    # Supported resolution for B09 is 60m\n",
    "    # Supported resolution for SCL is 20m\n",
    "    # SCL is the Scene Classification Layer that provides information about the scene such as clouds, water, etc.\n",
    "    # SCL Value\tMeaning\n",
    "    # 0\tNo Data\n",
    "    # 1\tSaturated/Defective\n",
    "    # 2\tDark Area Pixels\n",
    "    # 3\tCloud Shadow\n",
    "    # 4\tVegetation\n",
    "    # 5\tBare Soils\n",
    "    # 6\tWater\n",
    "    # 7\tClouds low probability / Unclassified\n",
    "    # 8\tClouds medium probability\n",
    "    # 9\tClouds high probability\n",
    "    # 10\tThin Cirrus\n",
    "    # 11\tSnow or Ice\n",
    "    resolution = 10 # Band Resolution\n",
    "\n",
    "    all_bands_evalscript = f\"\"\"\n",
    "    //VERSION=3\n",
    "\n",
    "    function setup() {{\n",
    "        return {{\n",
    "            input: [{{\n",
    "                bands: [\"B01\", \"B02\", \"B03\", \"B04\", \"B05\", \"B06\", \"B07\", \"B08\", \"B8A\", \"B09\", \"B11\", \"B12\", \"SCL\"],\n",
    "                units: \"{bands_units}\"\n",
    "            }}],\n",
    "            output: {{\n",
    "                bands: {bands_num},\n",
    "                sampleType: \"{sampleType}\"\n",
    "            }}\n",
    "        }};\n",
    "    }}\n",
    "\n",
    "    function evaluatePixel(sample) {{\n",
    "        return [\n",
    "            sample.B01, sample.B02, sample.B03, sample.B04, sample.B05, sample.B06,\n",
    "            sample.B07, sample.B08, sample.B8A, sample.B09, sample.B11, sample.B12,\n",
    "            sample.SCL\n",
    "        ];\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "    # Store requests to download\n",
    "    # Note: The requests are not executed yet, they are just stored in a list\n",
    "    process_requests = []\n",
    "\n",
    "    # Check if image already exists for unique acquisitions and specified resolution\n",
    "    for timestamp in unique_acquisitions:\n",
    "        iso_time = timestamp.isoformat().replace(\":\", \"\").replace(\"-\", \"\")\n",
    "        base_filename = f\"patch_{patch_id}_{iso_time}_res{resolution}\"\n",
    "        filename = os.path.join(save_dir, f\"{base_filename}.tiff\")\n",
    "\n",
    "        # Check if the file already exists\n",
    "        if not os.path.exists(filename):\n",
    "            request = SentinelHubRequest(\n",
    "                evalscript=all_bands_evalscript,\n",
    "                input_data=[\n",
    "                    SentinelHubRequest.input_data(\n",
    "                        data_collection=DataCollection.SENTINEL2_L2A.define_from(\"s2l2a\", service_url=config.sh_base_url),\n",
    "                        time_interval=(timestamp - time_difference, timestamp + time_difference),\n",
    "                    )\n",
    "                ],\n",
    "                responses=[SentinelHubRequest.output_response(\"default\", MimeType.TIFF)],  # Use TIFF for multi-band data\n",
    "                bbox=patch_bbox,\n",
    "                size=bbox_to_dimensions(patch_bbox, resolution=resolution),  # Adjust resolution as needed\n",
    "                config=config,\n",
    "            )\n",
    "            process_requests.append(request)\n",
    "        else:\n",
    "            print(f\"File {filename} already exists. Skipping download.\")\n",
    "\n",
    "    print(\"process_requests:\", len(process_requests))\n",
    "\n",
    "    # Download the images\n",
    "    client = SentinelHubDownloadClient(config=config)\n",
    "\n",
    "    download_requests = [request.download_list[0] for request in process_requests]\n",
    "\n",
    "    if not download_requests:\n",
    "        print(f\"No new images to download for patch {patch_id}\")\n",
    "        continue\n",
    "\n",
    "    # Download the images\n",
    "    data = client.download(download_requests)\n",
    "\n",
    "    # Example: Check dtype of each image in the downloaded data list\n",
    "    for i, array in enumerate(data):\n",
    "        print(f\"Image {i}: shape={array.shape}, dtype={array.dtype}\")\n",
    "\n",
    "    print(data)\n",
    "\n",
    "    ###############################\n",
    "    ###### PLOTTING & SAVING  #####\n",
    "    ###############################\n",
    "\n",
    "    # Configuration\n",
    "    pixel_scaling = 1\n",
    "    base_raw_dir = \"../data/raw/sentinel2\"\n",
    "    base_proc_dir = \"../data/processed/sentinel2\"\n",
    "    os.makedirs(base_raw_dir, exist_ok=True)\n",
    "    os.makedirs(base_proc_dir, exist_ok=True)\n",
    "\n",
    "    # Bounding box info (used in metadata, not folder naming)\n",
    "    ul_lon = round(patch_bbox.lower_left[0], 4)\n",
    "    ul_lat = round(patch_bbox.upper_right[1], 4)\n",
    "    br_lon = round(patch_bbox.upper_right[0], 4)\n",
    "    br_lat = round(patch_bbox.lower_left[1], 4)\n",
    "\n",
    "    # Define band list and original resolutions\n",
    "    band_list = [\"B01\", \"B02\", \"B03\", \"B04\", \"B05\", \"B06\", \"B07\", \"B08\", \"B8A\", \"B09\", \"B11\", \"B12\", \"SCL\"]\n",
    "    original_band_res = {\n",
    "        \"B01\": 60, \"B02\": 10, \"B03\": 10, \"B04\": 10,\n",
    "        \"B05\": 20, \"B06\": 20, \"B07\": 20,\n",
    "        \"B08\": 10, \"B8A\": 20, \"B09\": 60,\n",
    "        \"B11\": 20, \"B12\": 20, \"SCL\": 20\n",
    "    }\n",
    "\n",
    "    def normalize_by_scl(image, scl, valid_scl_values=[4, 5]):\n",
    "        \"\"\"\n",
    "        Normalize image using percentile scaling over valid SCL mask.\n",
    "        Returns normalized image, valid mask, list of p2, and p98 per band.\n",
    "        \"\"\"\n",
    "        valid_mask = np.isin(scl, valid_scl_values)\n",
    "        norm_image = np.zeros_like(image, dtype=np.float32)\n",
    "        norm_p2, norm_p98 = [], []\n",
    "\n",
    "        for band in range(image.shape[2]):\n",
    "            band_data = image[:, :, band]\n",
    "            band_valid = band_data[valid_mask]\n",
    "            if band_valid.size > 0:\n",
    "                p2, p98 = np.percentile(band_valid, (2, 98))\n",
    "            else:\n",
    "                p2, p98 = 0, 1\n",
    "            norm_p2.append(float(p2))\n",
    "            norm_p98.append(float(p98))\n",
    "            band_data = np.clip(band_data, p2, p98)\n",
    "            band_data = (band_data - p2) / (p98 - p2 + 1e-6)\n",
    "            norm_image[:, :, band] = band_data\n",
    "\n",
    "        return norm_image, valid_mask, norm_p2, norm_p98\n",
    "\n",
    "    # Save each image and metadata\n",
    "    for idx, (image, timestamp) in enumerate(zip(data, unique_acquisitions)):\n",
    "        if not isinstance(timestamp, dt.date):\n",
    "            print(f\"Invalid timestamp at index {idx}. Skipping image.\")\n",
    "            continue\n",
    "\n",
    "        # Generate unique filename\n",
    "        # File naming\n",
    "        short_uuid = str(uuid.uuid4())[:8]\n",
    "        iso_time = timestamp.isoformat().replace(\":\", \"\").replace(\"-\", \"\")\n",
    "        # base_filename = f\"{short_uuid}_{iso_time}_res{resolution}\"\n",
    "        base_filename = f\"patch_{patch_id}_{timestamp.isoformat().replace(':', '').replace('-', '')}_res{resolution}\"\n",
    "\n",
    "        # File paths\n",
    "        raw_tiff_path = os.path.join(base_raw_dir, f\"{base_filename}.tiff\")\n",
    "        proc_tiff_path = os.path.join(base_proc_dir, f\"{base_filename}.tiff\")\n",
    "        json_path = os.path.join(base_proc_dir, f\"{base_filename}.json\")  # metadata with processed version\n",
    "\n",
    "        image = image.astype(np.float32)\n",
    "\n",
    "        # Extract SCL and spectral bands\n",
    "        scl = image[:, :, -1]\n",
    "        spectral = image[:, :, :-1]\n",
    "\n",
    "        # Normalize spectral bands using valid SCL pixels\n",
    "        valid_scl_values = [4, 5]\n",
    "        norm_image, valid_mask, norm_p2, norm_p98 = normalize_by_scl(spectral, scl, valid_scl_values=valid_scl_values)\n",
    "\n",
    "        # ### PLOTTING ONLY ###\n",
    "        # # Visualize RGB with pixel scaling\n",
    "        # if norm_image.shape[2] >= 4:\n",
    "        #     rgb_indices = [3, 2, 1]  # B04, B03, B02\n",
    "        #     rgb_image = norm_image[:, :, rgb_indices] * pixel_scaling\n",
    "        #     rgb_image[~valid_mask] = 1.0  # Set invalid (cloud) pixels to white\n",
    "        #     plt.imshow(rgb_image)\n",
    "        #     plt.title(\"True Color (masked using SCL)\")\n",
    "        #     plt.axis(\"off\")\n",
    "        #     plt.show()\n",
    "        # else:\n",
    "        #     print(f\"Warning: Image at index {idx} doesn't have 3+ bands. Skipping display.\")\n",
    "\n",
    "        # # Save raw TIFF (unmodified reflectance + SCL)\n",
    "        # try:\n",
    "        #     image_raw = np.transpose(image, (2, 0, 1))  # (bands, height, width)\n",
    "        #     tifffile.imwrite(raw_tiff_path, image_raw)\n",
    "        #     print(f\"Saved RAW: {raw_tiff_path}\")\n",
    "        # except Exception as e:\n",
    "        #     print(f\"Error saving raw image at index {idx}: {e}\")\n",
    "        #     continue\n",
    "\n",
    "        # Save processed TIFF (normalized reflectance only, excluding SCL)\n",
    "        try:\n",
    "            image_norm = np.transpose(norm_image, (2, 0, 1))  # (bands, height, width)\n",
    "            tifffile.imwrite(proc_tiff_path, image_norm)\n",
    "            print(f\"Saved PROCESSED: {proc_tiff_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving processed image at index {idx}: {e}\")\n",
    "            # Log the failure\n",
    "            log_df = pd.concat([log_df, pd.DataFrame([{\n",
    "                \"patch_id\": patch_id,\n",
    "                \"uuid\": short_uuid,\n",
    "                \"timestamp\": timestamp.isoformat(),\n",
    "                \"file_processed\": os.path.basename(proc_tiff_path),\n",
    "                \"status\": f\"error: {str(e)[:100]}\"\n",
    "            }])], ignore_index=True)\n",
    "            log_df.to_csv(logfile_path, index=False)\n",
    "\n",
    "            continue\n",
    "\n",
    "        # Save metadata\n",
    "        metadata = {\n",
    "            \"source\": \"SENTINEL2_L2A\",\n",
    "            \"patch_id\": patch_id,\n",
    "            \"uuid\": short_uuid,\n",
    "            \"timestamp\": timestamp.isoformat(),\n",
    "            \"resolution\": resolution,\n",
    "            \"sampleType\": sampleType,\n",
    "            \"bands_units\": bands_units,\n",
    "            \"bands_num\": bands_num,\n",
    "            \"bands_shape\": image.shape,\n",
    "            \"bands\": band_list,\n",
    "            \"bbox\": {\n",
    "                \"upper_left\": [ul_lat, ul_lon],\n",
    "                \"bottom_right\": [br_lat, br_lon]\n",
    "            },\n",
    "            \"original_band_resolutions\": original_band_res,\n",
    "            \"cloud_mask_applied\": True,\n",
    "            \"max_search_cloud_cover\": cloud_cover_filter,\n",
    "            \"normalization\": {\n",
    "                \"percentiles\": [2, 98],\n",
    "                \"p2\": norm_p2,\n",
    "                \"p98\": norm_p98\n",
    "            },\n",
    "            \"file_raw\": os.path.basename(raw_tiff_path),\n",
    "            \"file_processed\": os.path.basename(proc_tiff_path),\n",
    "        }\n",
    "\n",
    "        with open(json_path, \"w\") as f:\n",
    "            json.dump(metadata, f, indent=4)\n",
    "            print(f\"Saved metadata: {json_path}\")\n",
    "\n",
    "        # Append log entry for this image\n",
    "        log_df = pd.concat([log_df, pd.DataFrame([{\n",
    "            \"patch_id\": patch_id,\n",
    "            \"uuid\": short_uuid,\n",
    "            \"timestamp\": timestamp.isoformat(),\n",
    "            \"file_processed\": os.path.basename(proc_tiff_path),\n",
    "            \"status\": \"success\"\n",
    "        }])], ignore_index=True)\n",
    "\n",
    "        # Save log immediately to avoid losing progress\n",
    "        log_df.to_csv(logfile_path, index=False)\n",
    "        print(f\"Logged patch {patch_id} to {logfile_path}\")\n",
    "\n",
    "\n",
    "    ### \"OUTER LOOP\"\n",
    "    # Print first image shape\n",
    "    if data:\n",
    "        print(f\"Shape of the first image: {data[0].shape}\")\n",
    "    else:\n",
    "        print(\"No data available.\")\n",
    "\n",
    "    # ######################################\n",
    "    # ### Plotting Multispectral Bands #####\n",
    "    # ######################################\n",
    "\n",
    "    # # Band order including SCL as last band (index 12)\n",
    "    # band_order = [\"B01\", \"B02\", \"B03\", \"B04\", \"B05\", \"B06\",\n",
    "    #               \"B07\", \"B08\", \"B8A\", \"B09\", \"B11\", \"B12\", \"SCL\"]\n",
    "    # numbered_band_order = {index: band for index, band in enumerate(band_order)}\n",
    "\n",
    "    # def extract_bands(image, band_names, band_dict):\n",
    "    #     \"\"\"\n",
    "    #     Extract specific bands from image based on band names.\n",
    "    #     \"\"\"\n",
    "    #     indices = [idx for name in band_names for idx, b in band_dict.items() if b == name]\n",
    "    #     return image[:, :, indices]\n",
    "\n",
    "    # def normalize_by_scl(image, scl, valid_scl_values=[4, 5]):\n",
    "    #     \"\"\"\n",
    "    #     Normalize image using percentile scaling over valid SCL mask.\n",
    "    #     Returns normalized image and the mask used.\n",
    "    #     \"\"\"\n",
    "    #     valid_mask = np.isin(scl, valid_scl_values)\n",
    "    #     norm_image = np.zeros_like(image, dtype=np.float32)\n",
    "\n",
    "    #     for band in range(image.shape[2]):\n",
    "    #         band_data = image[:, :, band]\n",
    "    #         band_valid = band_data[valid_mask]\n",
    "    #         if band_valid.size > 0:\n",
    "    #             p2, p98 = np.percentile(band_valid, (2, 98))\n",
    "    #         else:\n",
    "    #             p2, p98 = 0, 1\n",
    "    #         band_data = np.clip(band_data, p2, p98)\n",
    "    #         band_data = (band_data - p2) / (p98 - p2 + 1e-6)\n",
    "    #         norm_image[:, :, band] = band_data\n",
    "\n",
    "    #     return norm_image, valid_mask\n",
    "\n",
    "    # # ---- Load your DN image (shape: H, W, 13), for example:\n",
    "    # # image = tifffile.imread(path).transpose(1, 2, 0)\n",
    "\n",
    "    # # Split into spectral + SCL\n",
    "    # spectral = image[:, :, :-1]\n",
    "    # scl = image[:, :, -1]\n",
    "\n",
    "    # # Normalize using SCL land pixels\n",
    "    # norm_image, valid_mask = normalize_by_scl(spectral, scl)\n",
    "\n",
    "    # # Extract different band composites\n",
    "    # rgb = norm_image[:, :, [band_order.index(b) for b in [\"B04\", \"B03\", \"B02\"]]]\n",
    "    # vegetation = norm_image[:, :, [band_order.index(b) for b in [\"B08\", \"B04\", \"B03\"]]]\n",
    "    # urban = norm_image[:, :, [band_order.index(b) for b in [\"B12\", \"B11\", \"B04\"]]]\n",
    "    # water = norm_image[:, :, [band_order.index(b) for b in [\"B08\", \"B11\", \"B02\"]]]\n",
    "\n",
    "    # # Optional: Mask out invalid areas (e.g., clouds) as white\n",
    "    # rgb[~valid_mask] = 1.0\n",
    "    # vegetation[~valid_mask] = 1.0\n",
    "    # urban[~valid_mask] = 1.0\n",
    "    # water[~valid_mask] = 1.0\n",
    "\n",
    "    # # ---- Plot\n",
    "    # fig, axes = plt.subplots(2, 2, figsize=(15, 15))\n",
    "\n",
    "    # axes[0, 0].imshow(rgb)\n",
    "    # axes[0, 0].set_title(\"True Color (B04, B03, B02)\")\n",
    "    # axes[0, 0].axis('off')\n",
    "\n",
    "    # axes[0, 1].imshow(vegetation)\n",
    "    # axes[0, 1].set_title(\"False Color Vegetation (B08, B04, B03)\")\n",
    "    # axes[0, 1].axis('off')\n",
    "\n",
    "    # axes[1, 0].imshow(urban)\n",
    "    # axes[1, 0].set_title(\"Urban/Buildings (B12, B11, B04)\")\n",
    "    # axes[1, 0].axis('off')\n",
    "\n",
    "    # axes[1, 1].imshow(water)\n",
    "    # axes[1, 1].set_title(\"Water Detection (B08, B11, B02)\")\n",
    "    # axes[1, 1].axis('off')\n",
    "\n",
    "    # plt.tight_layout()\n",
    "    # plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
