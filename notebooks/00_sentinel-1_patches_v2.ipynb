{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021b3601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 00_sentinel-1_patches_v2.ipynb\n",
    "\n",
    "import os\n",
    "import time\n",
    "import uuid\n",
    "import json\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import datetime as dt\n",
    "from dotenv import load_dotenv\n",
    "from sentinelhub import (\n",
    "    SHConfig, SentinelHubCatalog, SentinelHubRequest, DataCollection, \n",
    "    MimeType, SentinelHubDownloadClient, BBox, CRS, bbox_to_dimensions, filter_times\n",
    ")\n",
    "from shapely.geometry import shape\n",
    "import tifffile\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configure Sentinel Hub\n",
    "config = SHConfig()\n",
    "config.sh_client_id = os.getenv(\"SENTINELHUB_CLIENT_ID3\")\n",
    "config.sh_client_secret = os.getenv(\"SENTINELHUB_CLIENT_SECRET3\")\n",
    "config.sh_base_url = 'https://sh.dataspace.copernicus.eu'\n",
    "config.sh_token_url = 'https://identity.dataspace.copernicus.eu/auth/realms/CDSE/protocol/openid-connect/token'\n",
    "\n",
    "# Define Sentinel-1 IW from CDSE endpoint\n",
    "data_collection = DataCollection.SENTINEL1_IW.define_from(\n",
    "    name=\"s1iw-cdse\",\n",
    "    service_url=config.sh_base_url\n",
    ")\n",
    "\n",
    "# Directories\n",
    "patches_path = \"../data/patches/highlighted_patches.geojson\"\n",
    "raw_dir = \"../data/raw/sentinel1\"\n",
    "proc_dir = \"../data/processed/sentinel1\"\n",
    "log_path = \"../data/logs/patch_download_log.csv\"\n",
    "os.makedirs(raw_dir, exist_ok=True)\n",
    "os.makedirs(proc_dir, exist_ok=True)\n",
    "os.makedirs(os.path.dirname(log_path), exist_ok=True)\n",
    "\n",
    "# Load patches\n",
    "patches_gdf = gpd.read_file(patches_path)\n",
    "print(f\"Loaded {len(patches_gdf)} patches\")\n",
    "\n",
    "# Sort patches\n",
    "patches_gdf[\"patch_index\"] = patches_gdf[\"patch_id\"].str.extract(r'patch_(\\d{5})')[0].astype(int)\n",
    "patches_gdf = patches_gdf.sort_values(\"patch_index\").reset_index(drop=True)\n",
    "\n",
    "# Resume support\n",
    "start_from_patch = None\n",
    "if start_from_patch and start_from_patch in patches_gdf[\"patch_id\"].values:\n",
    "    start_index = patches_gdf[patches_gdf[\"patch_id\"] == start_from_patch].index[0]\n",
    "    patches_gdf = patches_gdf.iloc[start_index:].reset_index(drop=True)\n",
    "\n",
    "# Set parameters\n",
    "resolution = 1024\n",
    "time_interval = (\"2010-01-01\", \"2030-12-31\")\n",
    "bands = [\"VV\"]\n",
    "bands_units = None\n",
    "bands_num = 1\n",
    "sampleType = \"FLOAT32\"\n",
    "catalog = SentinelHubCatalog(config=config)\n",
    "\n",
    "# Evalscript for Sentinel-1 VV band - Removed units: \"{bands_units}\"\n",
    "evalscript = f\"\"\"\n",
    "//VERSION=3\n",
    "function setup() {{\n",
    "  return {{\n",
    "    input: [{{\n",
    "      bands: [\"VV\"],\n",
    "    }}],\n",
    "    output: {{\n",
    "      bands: {bands_num},\n",
    "      sampleType: \"{sampleType}\"\n",
    "    }}\n",
    "  }};\n",
    "}}\n",
    "function evaluatePixel(sample) {{\n",
    "  return [sample.VV];\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "# Function to normalize SAR image\n",
    "def normalize_sentinel1(image, p2=2, p98=98):\n",
    "    image = image.astype(np.float32)\n",
    "    norm_image = np.zeros_like(image)\n",
    "    low_list, high_list = [], []\n",
    "    for i in range(image.shape[2]):\n",
    "        band = image[:, :, i]\n",
    "        low, high = np.percentile(band, (p2, p98))\n",
    "        band = np.clip(band, low, high)\n",
    "        norm_image[:, :, i] = (band - low) / (high - low + 1e-6)\n",
    "        low_list.append(float(low))\n",
    "        high_list.append(float(high))\n",
    "    return norm_image, low_list, high_list\n",
    "\n",
    "# Progress tracking\n",
    "n_patches = len(patches_gdf)\n",
    "completed = 0\n",
    "start_time = time.time()\n",
    "\n",
    "# Main loop\n",
    "for idx, row in patches_gdf.iterrows():\n",
    "    patch_id = row[\"patch_id\"]\n",
    "    geom = row[\"geometry\"]\n",
    "\n",
    "    if geom.geom_type != \"Polygon\":\n",
    "        print(f\"Skipping non-polygon geometry in patch {patch_id}\")\n",
    "        continue\n",
    "\n",
    "    patch_bbox = BBox(bbox=geom.bounds, crs=CRS.POP_WEB)\n",
    "    print(f\"\\n[{idx+1}/{n_patches}] Processing {patch_id} with bbox {patch_bbox}\")\n",
    "\n",
    "    try:\n",
    "        search_iterator = catalog.search(\n",
    "            data_collection,\n",
    "            bbox=patch_bbox,\n",
    "            time=time_interval,\n",
    "            fields={\"include\": [\"id\", \"properties.datetime\"], \"exclude\": []}\n",
    "        )\n",
    "        timestamps = search_iterator.get_timestamps()\n",
    "        unique_times = filter_times(timestamps, dt.timedelta(hours=6))\n",
    "\n",
    "        if not unique_times:\n",
    "            print(f\"No acquisitions found for {patch_id}\")\n",
    "            continue\n",
    "\n",
    "        client = SentinelHubDownloadClient(config=config)\n",
    "\n",
    "        # Add before loop\n",
    "        total_dates = len(unique_times)\n",
    "        date_count = 0\n",
    "        patch_start = time.time()\n",
    "\n",
    "        for timestamp in unique_times:\n",
    "            uid = str(uuid.uuid4())[:8]\n",
    "            iso_time = timestamp.isoformat().replace(\":\", \"\").replace(\"-\", \"\")\n",
    "            fname_base = f\"{patch_id}_{iso_time}_res{resolution}\"\n",
    "            raw_path = os.path.join(raw_dir, f\"{fname_base}.tiff\")\n",
    "            proc_path = os.path.join(proc_dir, f\"{fname_base}.tiff\")\n",
    "            json_path = os.path.join(proc_dir, f\"{fname_base}.json\")\n",
    "\n",
    "            # if os.path.exists(raw_path) and os.path.exists(proc_path) and os.path.exists(json_path):\n",
    "            #     print(f\"Image already exists for {patch_id} at {timestamp}\")\n",
    "            #     continue\n",
    "\n",
    "            if os.path.exists(raw_path) and os.path.exists(proc_path) and os.path.exists(json_path):\n",
    "                print(f\"[{date_count+1}/{total_dates}] ‚è© Skipped {timestamp} for {patch_id}\")\n",
    "                date_count += 1\n",
    "                continue\n",
    "\n",
    "            request = SentinelHubRequest(\n",
    "                evalscript=evalscript,\n",
    "                input_data=[\n",
    "                    SentinelHubRequest.input_data(\n",
    "                        data_collection=data_collection,\n",
    "                        time_interval=(timestamp - dt.timedelta(hours=6), timestamp + dt.timedelta(hours=6))\n",
    "                    )\n",
    "                ],\n",
    "                responses=[SentinelHubRequest.output_response(\"default\", MimeType.TIFF)],\n",
    "                bbox=patch_bbox,\n",
    "                size=(resolution, resolution),\n",
    "                config=config\n",
    "            )\n",
    "\n",
    "            image = None\n",
    "            for attempt in range(5):\n",
    "                try:\n",
    "                    print(f\"Downloading {patch_id} at {timestamp} (attempt {attempt+1})\")\n",
    "                    image = client.download(request.download_list[0])\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    print(f\"Attempt {attempt+1} failed: {e}\")\n",
    "                    time.sleep(2 ** attempt)\n",
    "\n",
    "            if image is None:\n",
    "                continue\n",
    "\n",
    "            # Save raw image\n",
    "            # Ensure image has 3 dimensions\n",
    "            if image.ndim == 2:\n",
    "                image = image[:, :, np.newaxis]\n",
    "\n",
    "            image_raw = np.transpose(image, (2, 0, 1))\n",
    "            tifffile.imwrite(raw_path, image_raw)\n",
    "            print(f\"Saved raw image: {raw_path}\")\n",
    "\n",
    "            # Normalize and save processed image\n",
    "            norm_image, p2_list, p98_list = normalize_sentinel1(image)\n",
    "            image_norm = np.transpose(norm_image, (2, 0, 1))\n",
    "            tifffile.imwrite(proc_path, image_norm)\n",
    "            print(f\"Saved normalized image: {proc_path}\")\n",
    "\n",
    "            metadata = {\n",
    "                \"source\": \"SENTINEL1_IW\",\n",
    "                \"patch_id\": patch_id,\n",
    "                \"uuid\": uid,\n",
    "                \"timestamp\": timestamp.isoformat(),\n",
    "                \"resolution\": resolution,\n",
    "                \"bands\": bands,\n",
    "                \"units\": bands_units,\n",
    "                \"sampleTypeRaw\": sampleType,\n",
    "                \"sampleType\": \"float32\",\n",
    "                \"normalization\": {\n",
    "                    \"percentiles\": [2, 98],\n",
    "                    \"p2\": p2_list,\n",
    "                    \"p98\": p98_list\n",
    "                },\n",
    "                \"shape\": image.shape,\n",
    "                \"file_raw\": os.path.basename(raw_path),\n",
    "                \"file_processed\": os.path.basename(proc_path)\n",
    "            }\n",
    "\n",
    "            with open(json_path, \"w\") as f:\n",
    "                json.dump(metadata, f, indent=4)\n",
    "                print(f\"Saved metadata: {json_path}\")\n",
    "\n",
    "            date_count += 1\n",
    "            patch_elapsed = time.time() - patch_start\n",
    "            avg_per_date = patch_elapsed / date_count\n",
    "            eta_dates_left = (total_dates - date_count) * avg_per_date\n",
    "\n",
    "            print(f\"[{date_count}/{total_dates}] ‚úÖ Done {patch_id} @ {timestamp} | avg: {avg_per_date:.2f}s/date | ETA left: {eta_dates_left/60:.1f} min\")\n",
    "            time.sleep(1)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {patch_id}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Progress reporting\n",
    "    completed += 1\n",
    "    elapsed = time.time() - start_time\n",
    "    avg_time = elapsed / max(completed, 1)\n",
    "    remaining = (n_patches - (idx + 1)) * avg_time\n",
    "    print(f\"‚è± Completed {completed}/{n_patches} | ETA: {remaining/60:.1f} min\")\n",
    "    print(f\"üéâ Finished {patch_id}: {total_dates} timestamps processed in {patch_elapsed/60:.2f} min\\n\")\n",
    "\n",
    "print(\"‚úÖ Sentinel-1 patch processing complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47141668",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
